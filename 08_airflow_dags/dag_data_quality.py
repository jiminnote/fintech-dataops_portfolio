"""
Airflow DAG: ë°ì´í„° í’ˆì§ˆ ê²€ì¦ íŒŒì´í”„ë¼ì¸
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ë§¤ 6ì‹œê°„ë§ˆë‹¤ ë°ì´í„° í’ˆì§ˆì„ ì ê²€í•˜ê³  ì´ìƒ ì‹œ ì•Œë¦¼ì„ ë°œì†¡í•©ë‹ˆë‹¤.
BranchPythonOperatorë¥¼ í™œìš©í•œ ì¡°ê±´ë¶€ ì•Œë¦¼ ë¡œì§ í¬í•¨.
"""

from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import BranchPythonOperator, PythonOperator
from airflow.operators.empty import EmptyOperator
from airflow.utils.trigger_rule import TriggerRule

default_args = {
    "owner": "dataops",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=3),
}

dag = DAG(
    dag_id="quickpay_data_quality",
    default_args=default_args,
    description="QuickPay ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ (6ì‹œê°„ ì£¼ê¸°)",
    schedule_interval="0 */6 * * *",  # ë§¤ 6ì‹œê°„
    start_date=datetime(2025, 11, 15),
    catchup=False,
    tags=["quickpay", "quality", "monitoring"],
    max_active_runs=1,
)

# â”â”â” Task 1: ì´ë²¤íŠ¸ ë³¼ë¥¨ ì²´í¬ â”â”â”
check_event_volume = BashOperator(
    task_id="check_event_volume",
    bash_command="""
        cd /opt/airflow/dags/fintech-dataops-portfolio
        python -c "
import duckdb
from datetime import datetime, timedelta

con = duckdb.connect('data/quickpay.duckdb', read_only=True)

# ìµœê·¼ 24ì‹œê°„ ì´ë²¤íŠ¸ ìˆ˜
result = con.execute('''
    WITH daily_counts AS (
        SELECT 
            CAST(event_timestamp AS DATE) AS dt,
            COUNT(*) AS cnt
        FROM events
        GROUP BY 1
    ),
    stats AS (
        SELECT AVG(cnt) AS mean_cnt, STDDEV(cnt) AS std_cnt 
        FROM daily_counts
    )
    SELECT 
        d.dt, d.cnt, s.mean_cnt, s.std_cnt,
        ROUND((d.cnt - s.mean_cnt) / NULLIF(s.std_cnt, 0), 2) AS zscore
    FROM daily_counts d
    CROSS JOIN stats s
    ORDER BY d.dt DESC
    LIMIT 1
''').fetchone()

con.close()

dt, cnt, mean_cnt, std_cnt, zscore = result
print(f'Date: {dt}, Count: {cnt}, Mean: {mean_cnt:.0f}, Z-score: {zscore}')

if abs(zscore) > 3:
    raise ValueError(f'ğŸš¨ Event volume anomaly! Z-score: {zscore}')
else:
    print(f'âœ… Event volume normal (Z-score: {zscore})')
"
    """,
    dag=dag,
)

# â”â”â” Task 2: ê±°ë˜ ì„±ê³µë¥  ì²´í¬ â”â”â”
check_success_rate = BashOperator(
    task_id="check_success_rate",
    bash_command="""
        cd /opt/airflow/dags/fintech-dataops-portfolio
        python -c "
import duckdb

con = duckdb.connect('data/quickpay.duckdb', read_only=True)

# ìµœê·¼ ê±°ë˜ ì„±ê³µë¥ 
result = con.execute('''
    SELECT
        ROUND(COUNT(CASE WHEN status = 'completed' THEN 1 END) * 100.0 / COUNT(*), 2) AS success_rate,
        COUNT(*) AS total
    FROM transactions
    WHERE CAST(created_at AS DATE) = (SELECT MAX(CAST(created_at AS DATE)) FROM transactions)
''').fetchone()

con.close()

success_rate, total = result
print(f'Success Rate: {success_rate}%, Total Txns: {total}')

if success_rate < 85:
    raise ValueError(f'ğŸš¨ Transaction success rate too low: {success_rate}%')
else:
    print(f'âœ… Success rate OK ({success_rate}%)')
"
    """,
    dag=dag,
)

# â”â”â” Task 3: ìŠ¤í‚¤ë§ˆ ë³€ê²½ ê°ì§€ â”â”â”
check_schema_drift = BashOperator(
    task_id="check_schema_drift",
    bash_command="""
        cd /opt/airflow/dags/fintech-dataops-portfolio
        python -c "
import duckdb
import json
from pathlib import Path

con = duckdb.connect('data/quickpay.duckdb', read_only=True)

# í˜„ì¬ ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ
tables = ['events', 'transactions', 'users']
current_schema = {}

for table in tables:
    cols = con.execute(f'DESCRIBE {table}').fetchdf()
    current_schema[table] = {row['column_name']: row['column_type'] for _, row in cols.iterrows()}

con.close()

# ì´ì „ ìŠ¤í‚¤ë§ˆì™€ ë¹„êµ
schema_path = Path('data/schema_snapshot.json')
if schema_path.exists():
    with open(schema_path) as f:
        prev_schema = json.load(f)
    
    for table in tables:
        prev_cols = set(prev_schema.get(table, {}).keys())
        curr_cols = set(current_schema[table].keys())
        
        new_cols = curr_cols - prev_cols
        removed_cols = prev_cols - curr_cols
        
        if new_cols:
            print(f'âš ï¸ {table}: New columns detected: {new_cols}')
        if removed_cols:
            print(f'âš ï¸ {table}: Removed columns: {removed_cols}')
        if not new_cols and not removed_cols:
            print(f'âœ… {table}: Schema unchanged')

# í˜„ì¬ ìŠ¤í‚¤ë§ˆ ì €ì¥
with open(schema_path, 'w') as f:
    json.dump(current_schema, f, indent=2)
    
print('ğŸ“¸ Schema snapshot saved')
"
    """,
    dag=dag,
)

# â”â”â” Task 4: ì „ì²´ í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰ â”â”â”
run_full_quality_checks = BashOperator(
    task_id="run_full_quality_checks",
    bash_command="""
        cd /opt/airflow/dags/fintech-dataops-portfolio
        python 07_data_quality/run_quality_checks.py 2>&1
    """,
    dag=dag,
)

# â”â”â” Task 5: ê²°ê³¼ ë¶„ê¸° â”â”â”
def _decide_alert(**kwargs):
    """í’ˆì§ˆ ì ìˆ˜ì— ë”°ë¼ ì•Œë¦¼ ìˆ˜ì¤€ ê²°ì •"""
    import json
    from pathlib import Path
    
    report_dir = Path("/opt/airflow/dags/fintech-dataops-portfolio/07_data_quality/reports")
    report_files = sorted(report_dir.glob("quality_report_*.json"))
    
    if not report_files:
        return "alert_critical"
    
    with open(report_files[-1]) as f:
        report = json.load(f)
    
    score = report["quality_score"]
    
    if score < 80:
        return "alert_critical"
    elif score < 90:
        return "alert_warning"
    else:
        return "no_alert"


decide_alert = BranchPythonOperator(
    task_id="decide_alert",
    python_callable=_decide_alert,
    dag=dag,
)

# â”â”â” ì•Œë¦¼ íƒœìŠ¤í¬ë“¤ â”â”â”
alert_critical = PythonOperator(
    task_id="alert_critical",
    python_callable=lambda: print("ğŸ”´ CRITICAL: Data quality score below 80%! Immediate action required."),
    dag=dag,
)

alert_warning = PythonOperator(
    task_id="alert_warning",
    python_callable=lambda: print("ğŸŸ¡ WARNING: Data quality score below 90%. Review within business hours."),
    dag=dag,
)

no_alert = EmptyOperator(
    task_id="no_alert",
    dag=dag,
)

# í•©ë¥˜ ì§€ì 
quality_check_done = EmptyOperator(
    task_id="quality_check_done",
    trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
    dag=dag,
)

# â”â”â” DAG ì˜ì¡´ì„± â”â”â”
# ë³‘ë ¬ ì‹¤í–‰: ë³¼ë¥¨ + ì„±ê³µë¥  + ìŠ¤í‚¤ë§ˆ
[check_event_volume, check_success_rate, check_schema_drift] >> run_full_quality_checks

# ë¶„ê¸°
run_full_quality_checks >> decide_alert
decide_alert >> [alert_critical, alert_warning, no_alert]
[alert_critical, alert_warning, no_alert] >> quality_check_done
