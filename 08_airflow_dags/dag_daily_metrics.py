"""
Airflow DAG: ì¼ê°„ ì§€í‘œ íŒŒì´í”„ë¼ì¸
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ë§¤ì¼ 06:00 KSTì— ì‹¤í–‰ë˜ì–´ ì „ì¼ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

íŒŒì´í”„ë¼ì¸ íë¦„:
  ë°ì´í„° ê²€ì¦ â†’ dbt ëª¨ë¸ ì‹¤í–‰ â†’ dbt í…ŒìŠ¤íŠ¸ â†’ Tableau ë°ì´í„° ê°±ì‹  â†’ Slack ë¦¬í¬íŠ¸
"""

from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import BranchPythonOperator, PythonOperator
from airflow.utils.trigger_rule import TriggerRule

# â”â”â” DAG ê¸°ë³¸ ì„¤ì • â”â”â”
default_args = {
    "owner": "dataops",
    "depends_on_past": False,
    "email": ["dataops@quickpay.com"],
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "execution_timeout": timedelta(hours=1),
}

dag = DAG(
    dag_id="quickpay_daily_metrics",
    default_args=default_args,
    description="QuickPay ì¼ê°„ KPI ì§€í‘œ íŒŒì´í”„ë¼ì¸",
    schedule_interval="0 21 * * *",  # UTC 21:00 = KST 06:00
    start_date=datetime(2025, 11, 15),
    catchup=False,
    tags=["quickpay", "metrics", "daily"],
    max_active_runs=1,
)

# â”â”â” Task 1: ë°ì´í„° ì‹ ì„ ë„ í™•ì¸ â”â”â”
check_data_freshness = BashOperator(
    task_id="check_data_freshness",
    bash_command="""
        cd /opt/airflow/dags/fintech-dataops-portfolio
        python -c "
import duckdb
from datetime import datetime, timedelta

con = duckdb.connect('data/quickpay.duckdb', read_only=True)

# ìµœì‹  ì´ë²¤íŠ¸ ì‹œê° í™•ì¸
latest = con.execute('''
    SELECT MAX(CAST(event_timestamp AS TIMESTAMP)) as latest_event
    FROM events
''').fetchone()[0]

# ìµœì‹  ê±°ë˜ ì‹œê° í™•ì¸
latest_txn = con.execute('''
    SELECT MAX(CAST(created_at AS TIMESTAMP)) as latest_txn
    FROM transactions
''').fetchone()[0]

con.close()

print(f'Latest event: {latest}')
print(f'Latest transaction: {latest_txn}')

# 24ì‹œê°„ ì´ë‚´ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
threshold = datetime.now() - timedelta(hours=48)
if latest and latest >= threshold:
    print('âœ… Data freshness OK')
else:
    raise ValueError(f'âš ï¸ Data is stale! Latest: {latest}, Threshold: {threshold}')
"
    """,
    dag=dag,
)

# â”â”â” Task 2: dbt ëª¨ë¸ ì‹¤í–‰ â”â”â”
run_dbt_models = BashOperator(
    task_id="run_dbt_models",
    bash_command="""
        cd /opt/airflow/dags/fintech-dataops-portfolio/04_dbt_mart
        dbt run --profiles-dir . --project-dir . 2>&1
    """,
    dag=dag,
)

# â”â”â” Task 3: dbt í…ŒìŠ¤íŠ¸ ì‹¤í–‰ â”â”â”
run_dbt_tests = BashOperator(
    task_id="run_dbt_tests",
    bash_command="""
        cd /opt/airflow/dags/fintech-dataops-portfolio/04_dbt_mart
        dbt test --profiles-dir . --project-dir . 2>&1
    """,
    dag=dag,
)

# â”â”â” Task 4: ë°ì´í„° í’ˆì§ˆ ê²€ì¦ â”â”â”
run_quality_checks = BashOperator(
    task_id="run_quality_checks",
    bash_command="""
        cd /opt/airflow/dags/fintech-dataops-portfolio
        python 07_data_quality/run_quality_checks.py 2>&1
    """,
    dag=dag,
)

# â”â”â” Task 5: í’ˆì§ˆ ê²°ê³¼ì— ë”°ë¥¸ ë¶„ê¸° â”â”â”
def _check_quality_result(**kwargs):
    """í’ˆì§ˆ ì ìˆ˜ì— ë”°ë¼ ë‹¤ìŒ ì‘ì—…ì„ ë¶„ê¸°"""
    import json
    from pathlib import Path
    
    report_dir = Path("/opt/airflow/dags/fintech-dataops-portfolio/07_data_quality/reports")
    report_files = sorted(report_dir.glob("quality_report_*.json"))
    
    if not report_files:
        return "notify_failure"
    
    with open(report_files[-1]) as f:
        report = json.load(f)
    
    # í’ˆì§ˆ ì ìˆ˜ 80% ë¯¸ë§Œì´ë©´ ì‹¤íŒ¨ ê²½ë¡œ
    if report["quality_score"] < 80:
        return "notify_failure"
    else:
        return "export_tableau_data"


branch_on_quality = BranchPythonOperator(
    task_id="branch_on_quality",
    python_callable=_check_quality_result,
    dag=dag,
)

# â”â”â” Task 6: Tableau ë°ì´í„° ê°±ì‹  â”â”â”
export_tableau_data = BashOperator(
    task_id="export_tableau_data",
    bash_command="""
        cd /opt/airflow/dags/fintech-dataops-portfolio
        python 06_tableau_dashboard/export_tableau_data.py 2>&1
    """,
    dag=dag,
)

# â”â”â” Task 7: ì„±ê³µ ì•Œë¦¼ â”â”â”
def _send_success_notification(**kwargs):
    """íŒŒì´í”„ë¼ì¸ ì„±ê³µ ì‹œ Slack ì•Œë¦¼"""
    import sys
    sys.path.insert(0, "/opt/airflow/dags/fintech-dataops-portfolio")
    from datetime import datetime
    
    # ê°„ë‹¨í•œ ì„±ê³µ ë©”ì‹œì§€
    execution_date = kwargs.get("ds", datetime.now().strftime("%Y-%m-%d"))
    print(f"âœ… Daily metrics pipeline completed successfully for {execution_date}")
    print("ğŸ“Š Tableau data exported. Dashboard will refresh automatically.")


notify_success = PythonOperator(
    task_id="notify_success",
    python_callable=_send_success_notification,
    dag=dag,
)

# â”â”â” Task 8: ì‹¤íŒ¨ ì•Œë¦¼ â”â”â”
def _send_failure_notification(**kwargs):
    """í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨ ì‹œ Slack ì•Œë¦¼"""
    print("ğŸ”´ Data quality check failed! Quality score below 80%.")
    print("ğŸ“‹ Please review the quality report and fix the issues.")


notify_failure = PythonOperator(
    task_id="notify_failure",
    python_callable=_send_failure_notification,
    dag=dag,
)

# â”â”â” DAG ì˜ì¡´ì„± â”â”â”
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
check_data_freshness >> run_dbt_models >> run_dbt_tests >> run_quality_checks

# ë¶„ê¸°
run_quality_checks >> branch_on_quality

# ì„±ê³µ ê²½ë¡œ
branch_on_quality >> export_tableau_data >> notify_success

# ì‹¤íŒ¨ ê²½ë¡œ
branch_on_quality >> notify_failure
